{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, os, glob, re, pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Globbing Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: Machine-Readable Central Asian Texts\n",
    "Corpus of complete texts edited by others composed in early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_corpus_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/Digital Humanities/\\\n",
    "Corpora/machine_readable_persian_transoxania_texts/*.txt')\n",
    "\n",
    "trans_corpus = {}\n",
    "for longname in trans_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_corpus[short[0]] = txt\n",
    "    \n",
    "    \n",
    "#trans_corpus['samarat']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: Machine-Readable Indo-Persian Texts\n",
    "Corpus of complete texts edited by others composed in early modern India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholder: /Users/Enkidu/Box\\ Sync/Notes/Digital\\ Humanities/Corpora/indo-persian_corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: Manuscript Notes\n",
    "Corpus based on partially transcribed manuscripts from early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmr_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/\\\n",
    "Primary Sources/non-machine-readable_notes/bactriana_notes/*.txt')\n",
    "\n",
    "raw_notes_corpus = {}\n",
    "for longname in nmr_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    raw_notes_corpus[short[0]] = txt\n",
    "\n",
    "#Adding in MarkDown stage files, not yet converted to XML    \n",
    "md_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/\\\n",
    "Primary Sources/transcription_markdown_drafting_stage1/document_conversion_backlog/pre-parser_backlog/**/*.txt', recursive=True)\n",
    "\n",
    "md_notes_corpus = {}\n",
    "for longname in md_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    md_notes_corpus[short[0]] = txt\n",
    "\n",
    "#raw_notes_corpus.keys()\n",
    "#md_notes_corpus.keys()\n",
    "#md_notes_corpus['apsa_524']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: XML Documents\n",
    "Corpus based on transcribed XML documents early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.5 and newer supports recursive **/ functionality, i.e. cycle through all subdirectories.\n",
    "\n",
    "xml_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/Primary Sources/xml_notes_stage2/**/*.xml', recursive=True)\n",
    "\n",
    "\n",
    "# For-loop through file names and build a dictionary of key (filename): value (text content)\n",
    "\n",
    "xml_corpus = {}\n",
    "for longname in xml_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_corpus[short[0]] = txt\n",
    "\n",
    "#xml_corpus['TsGARUz_i126_1_601_6_ser187']\n",
    "#xml_corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defunct method: [creating an NLTK corpus](http://www.nltk.org/book/ch02.html#loading-your-own-corpus)\n",
    "\n",
    "```python\n",
    "\n",
    "os.chdir('/Users/Enkidu/Documents/digital_humanities/jupyter_notebooks')\n",
    "corpus_root = 'machine_readable_persian_transoxania_texts'\n",
    "turkestan_corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "turkestan_corpus.fileids()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function replaces previous method (\"Not sure if you've seen dictionary comprehension before, but that there is it! Very useful. Saves you from having to do the explicit define dict then for loop rigamarole, and makes things more concise and readable.\" Previous method saved for posterity:\n",
    "\n",
    "```python\n",
    "clean_edited_i = {}\n",
    "for fn in raw_edited_corpus:\n",
    "    clean_edited_i[fn] = re.sub(r'ي', 'ی', raw_edited_corpus[fn])\n",
    "\n",
    "clean_edited = {}\n",
    "for fn in clean_edited_i:\n",
    "    clean_edited[fn] = re.sub(r'[^آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهس ي یی ]', '', clean_edited_i[fn])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From General Hanley\n",
    "\n",
    "def clean_document(doc):\n",
    "    doc = re.sub(r'ي', 'ی', doc)\n",
    "    doc = re.sub(r'[^آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهس ي یی ]', ' ', doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc)\n",
    "    doc = re.sub(r'ك', 'ک', doc)\n",
    "    doc = doc.strip()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning edited texts and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'جنابعالیمولایم سلمه الله متعالی جناب مرحمت و شفقت پناهی وزارتپناه مهربانی پروانچی حفظه الباری دام دولته معروض بر ضمیر منیر مهر تنویر خجسته مظاهر سیرات تخمیر عالیجاه رفیع جایگاه امیر الامرا انتظام محیت و م دت ارتسام اعنی مستغنی عن تعریف و توصیف را بمعرض قبول چنین رسانیده میشود که الحمد و المنه مجاری احوال بکرم و دولت بخیر بوده سلامتی ذات اشراف همایونی آبرومندی مطلوب مستدعااست نهفته و خفی مباد که والا جاها چنانجه درینآوان خرم نشان و از برای غلطانیده آمده بدریا بوده تدمانها رمیه نوازش پرور ارسالداشته بوده در بهترین زمان آمده رسیده بمطالعه پیوسته از مضامین بلاغت آیین او خورسندی بحصول پیوسته گردید امیدگاها احوال اینخدمت کار بجنابشان کما ینبغی معلوم میباشد از بیگاه اینجانب یکرم آلهی اندک تب و فراشاط ناک شده در طبعیت در طبیعت چیزی لاحظ در طبیعت چیزی لاحظ پیدا شد از اینوجه بقاضی و املاکداران تومان خط کرده آدم خود را همراه روانه نمودم دیگر اینکه مهربان پناها مبارکنامه فقرایان بجنابشان منظور شده گیست بمثل آن یک مبارکنامه حکم عرض کرده گرفته طیاری نموده مانند بهتر است گفته از روی نادانی انمعنی صلاح می افتیده باشد گفته آگاهی نموده فرستادم دیگر از دولت عالی بخاطر جمعی بوده دعا آب را بفقرایان نوبت دار کما کان رسانیده بذاتعای دعا گرفته استاده ایم ازینوجه خدمتکارانشان خوب مذمت کرده استاده اند فقر همه وقت بدعا یار نیز از دعای مقرون الاجابت اولا ذاتعالی طفیلی امیدوارم السلام'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean edited texts\n",
    "\n",
    "## TO DO: figure out more efficient way of doing the for loop, add in ک swaps too\n",
    "\n",
    "#Texts transcribed by other people\n",
    "clean_edited = {fn: clean_document(doc) for fn, doc in trans_corpus.items()}\n",
    "\n",
    "#XML-stage texts\n",
    "clean_xml = {fn: clean_document(doc) for fn, doc in xml_corpus.items()}\n",
    "\n",
    "#Raw Notes\n",
    "clean_notes = {fn: clean_document(doc) for fn, doc in raw_notes_corpus.items()}\n",
    "\n",
    "#Markdown Notes\n",
    "clean_markdown = {fn: clean_document(doc) for fn, doc in md_notes_corpus.items()}\n",
    "\n",
    "\n",
    "#clean_edited['ikromcha'][:1000]\n",
    "#clean_edited['ikromcha'][:1000]\n",
    "\n",
    "#clean_markdown['apsa_76']\n",
    "\n",
    "clean_xml['ser561']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning XML documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dormant XML cleaning method using BeautifulSoup (still in use for Persian literature tokenization in separate script)*\n",
    "\n",
    "```python\n",
    "bstree = bs4.BeautifulSoup(clean_xml[\"ser561\"], 'lxml')\n",
    "\n",
    "\n",
    "print(bstree.get_text())\n",
    "\n",
    "clean_xml = {}\n",
    "for fn in raw_xml:\n",
    "    bstree = bs4.BeautifulSoup(raw_xml[fn], 'lxml')\n",
    "    clean_xml[fn] = bstree.get_text()\n",
    "    \n",
    "clean_xml['TsGARUZ_i126_1_1986_1_ser201']\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edited_toks = {}\n",
    "for (fn, txt) in clean_edited.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    edited_toks[fn] = toks\n",
    "\n",
    "notes_toks = {}\n",
    "for (fn, txt) in clean_notes.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    notes_toks[fn] = toks\n",
    "    \n",
    "markdown_toks = {}\n",
    "for (fn, txt) in clean_markdown.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    notes_toks[fn] = toks\n",
    "\n",
    "\n",
    "xml_toks = {}\n",
    "for (fn, txt) in clean_xml.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    xml_toks[fn] = toks\n",
    "    \n",
    "\n",
    "#xml_toks['TsGARUz_R-2678_ser184'][50:70]\n",
    "\n",
    "#notes_toks[\"jung_i_mahzar_va_rivayat_al_biruni_9767\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Literature Digital Corpus\n",
    "Massive corpus of Persian literature, pulled from Ganjur (http://ganjoor.net/) by Roshan (https://persdigumd.github.io/PDL/)\n",
    "\n",
    "*Corpus pre-cleaned, tokenized, and pickled from a separate script. (Cleaning takes a long time; and this corpus doesn't change very often, and so does not need to be re-run.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/Users/Enkidu/Box Sync/Notes/Digital Humanities/Corpora/corpus_scripts/persian_lit_toks.pkl', 'rb') \n",
    "\n",
    "pers_lit_toks = pickle.load(f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pers_lit_toks.keys()\n",
    "#pers_lit_toks[\"hafez.masnavi\"]\n",
    "#pers_lit_toks['ferdowsi.shahnameh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging dictionaries: https://www.webucator.com/how-to/how-merge-dictionaries-python.cfm\n",
    "\n",
    "\n",
    "#Combined corpus\n",
    "combined_corpus_toks = {**edited_toks, **notes_toks, **xml_toks, **markdown_toks}\n",
    "\n",
    "\n",
    "#Combined Tokens (loses corpus text designation)\n",
    "combined_toks = []\n",
    "for (fn, text) in combined_corpus_toks.items():\n",
    "    combined_toks.extend(combined_corpus_toks[fn])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency: Combined Central Asia Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('اشتغالداشته', 6),\n",
       " ('جنابعالیحضرتم', 6),\n",
       " ('اجمعینتیمنا', 5),\n",
       " ('جنابعالیحضرت', 4),\n",
       " ('جنابعالیمولایم', 4),\n",
       " ('اشتغالنموده', 3),\n",
       " ('میگذرانیدند', 2),\n",
       " ('تورسونخواجه', 2),\n",
       " ('بآنعالیحضرت', 2),\n",
       " ('استقامتداشتند', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pah_freq = nltk.FreqDist(pah_toks_clean)\n",
    "#pah_freq.most_common(20)\n",
    "\n",
    "combo_freq = nltk.FreqDist(combined_toks)\n",
    "\n",
    "\n",
    "long_toks = [x for x in combined_toks if len(x)>10]\n",
    "long_freq = nltk.FreqDist(long_toks)\n",
    "long_freq.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency: Persian Lit Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('و', 24278),\n",
       " ('به', 15014),\n",
       " ('که', 12701),\n",
       " ('ز', 11879),\n",
       " ('از', 10015),\n",
       " ('بر', 7735),\n",
       " ('را', 7571),\n",
       " ('چو', 6406),\n",
       " ('با', 4634),\n",
       " ('گفت', 4152)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Placeholder\n",
    "\n",
    "ferdows = nltk.FreqDist(pers_lit_toks['ferdowsi.shahnameh'])\n",
    "ferdows.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('بعد', 'از'), 1082),\n",
       " (('و', 'از'), 753),\n",
       " (('و', 'در'), 669),\n",
       " (('اند', 'و'), 614),\n",
       " (('است', 'و'), 458),\n",
       " (('که', 'در'), 410),\n",
       " (('از', 'ان'), 406),\n",
       " (('قدس', 'سره'), 395),\n",
       " (('که', 'از'), 393),\n",
       " (('را', 'از'), 383)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = nltk.ngrams(combined_toks, 2)\n",
    "bi_freq = nltk.FreqDist(bigrams)\n",
    "bi_freq.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('بعد', 'از', 'ان'), 200),\n",
       " (('بوده', 'اند', 'و'), 124),\n",
       " (('الله', 'علیه', 'و'), 106),\n",
       " (('و', 'بعد', 'از'), 104),\n",
       " (('علیه', 'و', 'سلم'), 102),\n",
       " (('قدس', 'سر', 'ه'), 88),\n",
       " (('و', 'از', 'ایشان'), 86),\n",
       " (('فرموده', 'اند', 'ه'), 85),\n",
       " (('بشرایطه', 'یا', 'نی'), 81),\n",
       " (('ابو', 'الفیض', 'خان'), 74)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = nltk.ngrams(combined_toks, 3)\n",
    "tri_freq = nltk.FreqDist(trigrams)\n",
    "tri_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muchos-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "muchos_grams = nltk.ngrams(combined_toks, 10)\n",
    "muchos_freq = nltk.FreqDist(muchos_grams)\n",
    "#muchos_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConditionalFreqDist() takes a list of pairs.\n",
    "# Generator variable uses itself up upon assignment, so need to recreate above\n",
    "\n",
    "bigrams_cfd = nltk.ngrams(combined_toks, 2)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigrams_cfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('اولیا', 2), ('صلو', 1), ('ل', 1), ('شرف', 1), ('مبار', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd['خاتم'].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third word, if first two words known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri0 = nltk.ngrams(combined_toks, 3)\n",
    "tri1 = [((a, b), c) for (a, b, c) in tri0]\n",
    "cfd1 = nltk.ConditionalFreqDist(tri1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'ان': 200, 'آن': 67, 'چند': 44, 'وفات': 34, 'فراغ': 25, 'ختم': 25, 'فوت': 20, 'انکه': 18, 'نماز': 15, 'اتمام': 14, ...})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd1[(\"بعد\",\"از\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reversed conditional frequency, i.e. if second word in sequence known but not first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'در': 30, 'از': 12, 'هزار': 7, 'تب': 6, 'و': 6, 'آن': 5, 'اهل': 5, 'کتاب': 5, 'درس': 5, 'که': 4, ...})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi0 = nltk.ngrams(combined_toks, 2)\n",
    "bir = [(b, a) for (a, b) in bi0]\n",
    "cfdr = nltk.ConditionalFreqDist(bir)\n",
    "\n",
    "cfdr[\"خانه\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Concofdance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 7 of 7 matches:\n",
      "لم خواجه ام وکال مطلق عام از قبل هاجر بیگم و عایشه بیگم بنتی ملا محمد نظر سید بر\n",
      "کال مطلق عام از قبل هاجر بیگم و عایشه بیگم بنتی ملا محمد نظر سید برین حضر عبد ال\n",
      "ست که والده ایشان مسماه سلطانیم برسون بیگم بنت ایشان سید امیر صدر الدین خواجه از\n",
      "ر از قبیل خواهر میانه مرحومه ام صباحت بیگم نام و نور چشم ا و لیم محترمه جان و شو\n",
      "بیگ دویم مرزا ابراهیم بیک از بطن زمرد بیگم دعاگو دو فرزند یکی حرمت الله بیگ و دو\n",
      " بیگ و دویم مرزا کریم بیگ از بطن زمرد بیگم دعاگو و دو فرزند یکی زن منکوحه زوجه ح\n",
      "رداری دریاب رجب سنه دعا گو مسمات زمرد بیگم بنت میر مصطفی که زوجه مرزا حاجی بیگ و\n"
     ]
    }
   ],
   "source": [
    "# for whatever reason you can't just use the concordance method on a string;\n",
    "# you have to convert it to an NLTK Text type one way or another\n",
    "\n",
    "trans_corpus = nltk.Text(combined_toks)\n",
    "\n",
    "trans_corpus.concordance('بیگم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " آن زمین قطب اعظم این جهان را آفتاب بیگمان چون انصر پا ش ز ان مصطفا ست در حسب \n",
      "الرحمن محبوب الزمان سعید دوران شهید بیگمان حضرت میرزا جان جانان قدس سره در بلا\n",
      "م خواجه ام وکال مطلق عام از قبل هاجر بیگم و عایشه بیگم بنتی ملا محمد نظر سید ب\n",
      "ال مطلق عام از قبل هاجر بیگم و عایشه بیگم بنتی ملا محمد نظر سید برین حضر عبد ا\n",
      "ت که والده ایشان مسماه سلطانیم برسون بیگم بنت ایشان سید امیر صدر الدین خواجه ا\n",
      " از قبیل خواهر میانه مرحومه ام صباحت بیگم نام و نور چشم ا و لیم محترمه جان و ش\n",
      "یگ دویم مرزا ابراهیم بیک از بطن زمرد بیگم دعاگو دو فرزند یکی حرمت الله بیگ و د\n",
      "بیگ و دویم مرزا کریم بیگ از بطن زمرد بیگم دعاگو و دو فرزند یکی زن منکوحه زوجه \n",
      "داری دریاب رجب سنه دعا گو مسمات زمرد بیگم بنت میر مصطفی که زوجه مرزا حاجی بیگ \n"
     ]
    }
   ],
   "source": [
    "toks = [x for x in combo_freq if re.match(r'ب.گم', x)]\n",
    "conc0 = sum([trans_corpus.concordance_list(x) for x in toks], [])\n",
    "conc1 = [c.line for c in conc0]\n",
    "print('\\n'.join(conc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['بیگمان', 'بیگم']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34088"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combo_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
