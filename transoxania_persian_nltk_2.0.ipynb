{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, os, glob, re, bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Globbing Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: Machine-Readable Central Asian Texts\n",
    "Corpus of complete texts edited by others composed in early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_corpus_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/Digital Humanities/\\\n",
    "Corpora/machine_readable_persian_transoxania_texts/*.txt')\n",
    "\n",
    "trans_corpus = {}\n",
    "for longname in trans_corpus_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_corpus[short[0]] = txt\n",
    "    \n",
    "\n",
    "#trans_corpus['samarat']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: Manuscript Notes\n",
    "Corpus based on partially transcribed manuscripts from early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmr_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/\\\n",
    "Primary Sources/non-machine-readable_notes/bactriana_notes/*.txt')\n",
    "\n",
    "raw_notes_corpus = {}\n",
    "for longname in nmr_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    raw_notes_corpus[short[0]] = txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Build: XML Documents\n",
    "Corpus based on transcribed XML documents early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.5 and newer supports recursive **/ functionality, i.e. cycle through all subdirectories.\n",
    "\n",
    "xml_files = glob.glob(r'/Users/Enkidu/Box Sync/Notes/Primary Sources/xml_notes_stage2/**/*.xml', recursive=True)\n",
    "\n",
    "\n",
    "# For-loop through file names and build a dictionary of key (filename): value (text content)\n",
    "\n",
    "xml_corpus = {}\n",
    "for longname in xml_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_corpus[short[0]] = txt\n",
    "\n",
    "#xml_corpus['TsGARUz_i126_1_601_6_ser187']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defunct method: [creating an NLTK corpus](http://www.nltk.org/book/ch02.html#loading-your-own-corpus)\n",
    "\n",
    "```python\n",
    "\n",
    "os.chdir('/Users/Enkidu/Documents/digital_humanities/jupyter_notebooks')\n",
    "corpus_root = 'machine_readable_persian_transoxania_texts'\n",
    "turkestan_corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "turkestan_corpus.fileids()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning edited texts and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اتقیای رحم الله که هر یک مقتدای  دین راهنمای رسوم '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean edited texts\n",
    "\n",
    "## TO DO: figure out more efficient way of doing the for loop, add in ک swaps too\n",
    "\n",
    "clean_edited_i = {}\n",
    "for fn in raw_edited_corpus:\n",
    "    clean_edited_i[fn] = re.sub(r'ي', 'ی', raw_edited_corpus[fn])\n",
    "\n",
    "clean_edited = {}\n",
    "for fn in clean_edited_i:\n",
    "    clean_edited[fn] = re.sub(r'[^آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهس ي یی ]', '', clean_edited_i[fn])\n",
    "\n",
    "#clean_edited['ikromcha'][:1000]\n",
    "#raw_edited_corpus['ikromcha'][:1000]\n",
    "\n",
    "\n",
    "\n",
    "clean_notes_i = {}\n",
    "for fn in raw_notes_corpus:\n",
    "    clean_notes_i[fn] = re.sub(r'ي', 'ی', raw_notes_corpus[fn])\n",
    "\n",
    "clean_notes = {}\n",
    "for fn in clean_notes_i:\n",
    "    clean_notes[fn] = re.sub(r'[^آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهس ي یی ]', '', clean_notes_i[fn])\n",
    "\n",
    "clean_notes['jung_i_rivayat_al_biruni_4798'][600:650]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning XML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bstree = bs4.BeautifulSoup(raw_xml[\"ser561\"], 'lxml')\n",
    "\n",
    "#print(bstree.get_text())\n",
    "\n",
    "\n",
    "## TO DO: still need to strip out some of the non-Arabic script stuff, like above.\n",
    "\n",
    "clean_xml = {}\n",
    "for fn in raw_xml:\n",
    "    bstree = bs4.BeautifulSoup(raw_xml[fn], 'lxml')\n",
    "    clean_xml[fn] = bstree.get_text()\n",
    "    \n",
    "#clean_xml['TsGARUZ_i126_1_1986_1_ser201']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['عبد',\n",
       " 'الاحد',\n",
       " 'گذشته',\n",
       " 'گی',\n",
       " 'این',\n",
       " 'است',\n",
       " 'داملا',\n",
       " 'حسن',\n",
       " 'آخوند',\n",
       " 'بخاری',\n",
       " 'الاصل',\n",
       " 'از',\n",
       " 'گذر',\n",
       " 'کاکلهً',\n",
       " 'خورد',\n",
       " 'بسیار',\n",
       " 'ملا',\n",
       " 'درس',\n",
       " 'گو',\n",
       " 'بوده']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "edited_toks = {}\n",
    "for (fn, txt) in clean_edited.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    edited_toks[fn] = toks\n",
    "\n",
    "notes_toks = {}\n",
    "for (fn, txt) in clean_notes.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    notes_toks[fn] = toks\n",
    "\n",
    "\n",
    "xml_toks = {}\n",
    "for (fn, txt) in clean_xml.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    xml_toks[fn] = toks\n",
    "    \n",
    "\n",
    "xml_toks['TsGARUz_R-2678_ser184'][50:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging dictionaries: https://www.webucator.com/how-to/how-merge-dictionaries-python.cfm\n",
    "\n",
    "\n",
    "#Combined corpus\n",
    "combined_corpus_toks = {**edited_toks, **notes_toks, **xml_toks}\n",
    "\n",
    "\n",
    "#Combined Tokens (loses corpus text designation)\n",
    "combined_toks = []\n",
    "for (fn, text) in list(combined_corpus_toks.items()):\n",
    "    combined_toks.extend(combined_corpus_toks[fn])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Literature Digital Corpus\n",
    "Massive corpus of Persian literature, pulled from Ganjur (http://ganjoor.net/) by Roshan (https://persdigumd.github.io/PDL/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['University',\n",
       " 'of',\n",
       " 'Maryland',\n",
       " 'College',\n",
       " 'ParkGanjoor',\n",
       " 'corpus',\n",
       " 'به',\n",
       " 'نام',\n",
       " 'خداوند',\n",
       " 'جان',\n",
       " 'و',\n",
       " 'خرد',\n",
       " 'کزین',\n",
       " 'برتر',\n",
       " 'اندیشه',\n",
       " 'برنگذرد',\n",
       " 'خداوند',\n",
       " 'نام',\n",
       " 'و',\n",
       " 'خداوند']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ISSUE: A lot of the corpus is in Unicode format, some of it is in Arabic script. Is the unicode automatically converted?\n",
    "\n",
    "#Globbing\n",
    "\n",
    "files = glob.glob(r'/Users/Enkidu/Documents/digital_humanities/\\\n",
    "persian_literature_digital_corpus_roshan/data/**/*.xml', recursive=True)\n",
    "\n",
    "\n",
    "#Assembling Corpus\n",
    "\n",
    "raw_perslit_corpus = {}\n",
    "for longname in files:\n",
    "        f = open(longname)\n",
    "        txt = f.read()\n",
    "        f.close()\n",
    "        start = longname.rindex('/')+1\n",
    "        short = longname[start:-4]\n",
    "        raw_perslit_corpus[short] = txt\n",
    "\n",
    "        \n",
    "#Cleaning Text\n",
    "## TO DO: still need to strip out some of the non-Arabic script stuff, like above.\n",
    "\n",
    "\n",
    "clean_perslit = {}\n",
    "for fn in raw_perslit_corpus:\n",
    "    bstree = bs4.BeautifulSoup(raw_perslit_corpus[fn], 'lxml')\n",
    "    clean_perslit[fn] = bstree.get_text()\n",
    "\n",
    "#Tokenizing Text\n",
    " \n",
    "perslit_toks = {}\n",
    "for (fn, txt) in clean_perslit.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    perslit_toks[fn] = toks\n",
    "        \n",
    "#clean_perslit['ferdousi.shahname.pdl'][5000:5800]\n",
    "perslit_toks['ferdousi.shahname.pdl'][50:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('چنانچهجنانجه', 68),\n",
       " ('بزروارپزروار', 64),\n",
       " ('اینچنیناینجنین', 25),\n",
       " ('i126-1-1730', 7),\n",
       " ('اشتغالداشته', 6),\n",
       " ('بماوراالنهر', 5),\n",
       " ('همچنانهمجنان', 5),\n",
       " ('اجمعینتیمنا', 5),\n",
       " ('همچنینهمجنین', 4),\n",
       " ('اشتغالنموده', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pah_freq = nltk.FreqDist(pah_toks_clean)\n",
    "#pah_freq.most_common(20)\n",
    "\n",
    "combo_freq = nltk.FreqDist(combined_toks)\n",
    "\n",
    "\n",
    "long_toks = [x for x in combined_toks if len(x)>10]\n",
    "long_freq = nltk.FreqDist(long_toks)\n",
    "long_freq.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('بعد', 'از'), 1065),\n",
       " (('و', 'از'), 741),\n",
       " (('و', 'در'), 664),\n",
       " (('اند', 'و'), 613),\n",
       " (('قدس', 'سره'), 502),\n",
       " (('است', 'و'), 453),\n",
       " (('از', 'ان'), 391),\n",
       " (('که', 'در'), 379),\n",
       " (('را', 'از'), 363),\n",
       " (('بوده', 'اند'), 361)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = nltk.ngrams(combined_toks, 2)\n",
    "bi_freq = nltk.FreqDist(bigrams)\n",
    "bi_freq.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('بعد', 'از', 'ان'), 192),\n",
       " (('بوده', 'اند', 'و'), 124),\n",
       " (('الله', 'علیه', 'و'), 105),\n",
       " (('علیه', 'و', 'سلم'), 103),\n",
       " (('و', 'بعد', 'از'), 103),\n",
       " (('و', 'از', 'ایشان'), 86),\n",
       " (('بشرایطه', 'یا', 'نی'), 81),\n",
       " (('فرموده', 'اند', 'ه'), 80),\n",
       " (('درین', 'مسله', 'که'), 75),\n",
       " (('ابو', 'الفیض', 'خان'), 71)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = nltk.ngrams(combined_toks, 3)\n",
    "tri_freq = nltk.FreqDist(trigrams)\n",
    "tri_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muchos-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "muchos_grams = nltk.ngrams(combined_toks, 10)\n",
    "muchos_freq = nltk.FreqDist(muchos_grams)\n",
    "#muchos_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConditionalFreqDist() takes a list of pairs.\n",
    "# Generator variable uses itself up upon assignment, so need to recreate above\n",
    "\n",
    "## TO DO: How to integrate Regex searches into this?\n",
    "\n",
    "bigrams_cfd = nltk.ngrams(combined_toks, 2)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigrams_cfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('مسله', 80), ('باب', 36), ('اثنا', 27), ('آوان', 21), ('ایام', 18)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd['درین'].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "# for whatever reason you can't just use the concordance method on a string;\n",
    "# you have to convert it to an NLTK Text type one way or another\n",
    "\n",
    "trans_corpus = nltk.Text(combined_toks)\n",
    "\n",
    "trans_corpus.concordance('نقشب')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
