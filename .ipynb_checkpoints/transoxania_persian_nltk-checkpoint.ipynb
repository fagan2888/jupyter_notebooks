{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, os\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating NLTJK Corpus Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'ikromcha.txt', 'khumuli.txt', 'samarat.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://www.nltk.org/book/ch02.html#loading-your-own-corpus\n",
    "\n",
    "os.chdir('/Users/Enkidu/Documents/digital_humanities/jupyter_notebooks')\n",
    "corpus_root = 'machine_readable_persian_transoxania_texts'\n",
    "turkestan_corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "turkestan_corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مندرجه', 'ديباچه', '133b', '))', ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glob through the folder, create a list of strings\n",
    "\n",
    "\n",
    "turkestan_corpus.words(['khumuli.txt', 'ikromcha.txt', 'samarat.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*next steps*: still messing up boolean logic in latter part of the string comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "#pah_toks_clean = [x for x in pah_toks if x.isalpha() or '-' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pah_toks_clean = [x for x in pah_toks if not x.lower() == ('colophon' or 'n' or 'not')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pah_toks_clean = [x for x in pah_toks_clean if len(x) > 2]\n",
    "#print ('Number of clean tokens: ', str(len(pah_toks_clean)))\n",
    "#print ('Tokens reduced by', str(int(len(pah_toks_clean)/len(pah_toks)*100)), 'percent in cleaning process.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*next steps*: need to figure out how to better take advantage of the iterator (same for below) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pah_freq = nltk.FreqDist(pah_toks_clean)\n",
    "#pah_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pah_bi = nltk.ngrams(pah_toks_clean, 2)\n",
    "#pah_bi_freq = nltk.FreqDist(pah_bi)\n",
    "#pah_bi_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pah_tri = nltk.ngrams(pah_toks_clean, 3)\n",
    "#pah_tri_freq = nltk.FreqDist(pah_tri)\n",
    "#pah_tri_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConditionalFreqDist() takes a list of pairs.\n",
    "# Generator variable uses itself up upon assignment, so need to recreate above\n",
    "\n",
    "#pah_bi2 = nltk.ngrams(pah_toks_clean, 2)\n",
    "#pah_bi2 = list(pah_bi2)\n",
    "\n",
    "#pah_cfd = nltk.ConditionalFreqDist(pah_bi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pah_cfd['xurdruš'].most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for whatever reason you can't just use the concordance method on a string;\n",
    "# you have to convert it to an NLTK Text type one way or another\n",
    "\n",
    "#pahlavi_corpus = nltk.Text(pah_toks_clean)\n",
    "\n",
    "#pahlavi_corpus.concordance('mēnōy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
